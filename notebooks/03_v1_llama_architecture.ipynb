{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a9URHNu86QO",
        "outputId": "7096236d-25ea-4159-b550-0862b939bcac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.12/dist-packages (0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install torch dataclasses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List, Dict, Optional"
      ],
      "metadata": {
        "id": "uvP5_TW8Xnme"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, hidden_size: int, eps: float=1e-6):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "    self.variance_epsilon = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    variance = x.pow(2).mean(-1, keepdim=True)\n",
        "    hidden_states *= torch.rsqrt(variance + self.variance_epsilon)\n",
        "    return self.weight * hidden_states"
      ],
      "metadata": {
        "id": "JbMcZeAeX0p2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPE(nn.Module):\n",
        "  def __init__(self, dim: int, max_positional_embeddings: int, rope_theta: float = 10000.0):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.max_positional_embeddings = max_positional_embeddings\n",
        "    self.rope_theta = rope_theta\n",
        "\n",
        "    inv_freq = 1.0 / self.rope_theta ** (torch.arange(0, self.dim, 2).float() / self.dim)\n",
        "    self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, seq_len: int):\n",
        "    t = torch.arange(seq_len, device=x.device, dtype=self.inv_freq.dtype)\n",
        "    freqs = torch.outer(t, self.inv_freq)\n",
        "    emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "    cos = emb.cos()\n",
        "    sin = emb.sin()\n",
        "\n",
        "    return cos, sin"
      ],
      "metadata": {
        "id": "I2z-12qQZ0fg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_half(x):\n",
        "  x1 = x[..., :x.shape[-1] // 2]\n",
        "  x2 = x[..., x.shape[-1] // 2:]\n",
        "  return torch.cat((-x2, x1), dim=-1)"
      ],
      "metadata": {
        "id": "82WfKVYSa7XF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "  q_embed = (q*cos) + (rotate_half(q)*sin)\n",
        "  k_embed = (k*cos) + (rotate_half(k)*sin)\n",
        "  return q_embed, k_embed"
      ],
      "metadata": {
        "id": "gzAUR5JGbFvK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "  def __init__(self, config: Dict):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.num_heads = config.num_attention_heads\n",
        "    self.num_kv_heads = config.num_kv_heads\n",
        "    self.num_kv_groups = self.num_heads // self.num_kv_heads\n",
        "    self.head_dim = self.hidden_size // self.num_heads\n",
        "    self.attention_dropout = config.attention_dropout\n",
        "\n",
        "    self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
        "    self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)\n",
        "    self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads*self.head_dim, bias=False)\n",
        "    self.o_proj = nn.Linear(self.num_heads*self.head_dim, self.hidden_size, bias=False)\n",
        "\n",
        "    self.rope = RoPE(\n",
        "        self.head_dim,\n",
        "        max_positional_embeddings=config.max_positional_embeddings,\n",
        "        rope_theta=config.rope_theta\n",
        "    )\n",
        "  def forward(\n",
        "      self,\n",
        "      hidden_states: torch.Tensor,\n",
        "      attention_mask: Optional[torch.Tensor] = None,\n",
        "      position_ids: Optional[torch.Tensor] = None\n",
        "  ) -> torch.Tensor:\n",
        "    B, T, C = hidden_states.size()\n",
        "\n",
        "    query_states = self.q_proj(hidden_states).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    key_states = self.k_proj(hidden_states).view(B, T, self.num_kv_heads, self.head_dim).transpose(1,2)\n",
        "    value_states = self.v_proj(hidden_states).view(B, T, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    cos, sin = self.rope(value_states, seq_len=T)\n",
        "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
        "\n",
        "    if self.num_kv_groups > 1:\n",
        "      key_states = key_states.repeat_interleave(self.num_kv_groups, dim=1)\n",
        "      value_states = value_states.repeat_interleave(self.num_kv_groups, dim=1)\n",
        "\n",
        "    attention_weights = query_states @ key_states.transpose(2, 3)\n",
        "    attention_weights = attention_weights / math.sqrt(self.head_dim)\n",
        "\n",
        "    if attention_mask is not None:\n",
        "      attention_weights += attention_mask\n",
        "\n",
        "    attention_weights = F.softmax(attention_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "    attention_weights = F.dropout(attention_weights, p=self.attention_dropout, training=self.training)\n",
        "\n",
        "    attention_output = attention_weights @ value_states\n",
        "    attention_output = attention_output.transpose(1,2).contiguous().view(B, T, self.hidden_size)\n",
        "    attention_output = self.o_proj(attention_output)\n",
        "    return attention_output"
      ],
      "metadata": {
        "id": "PI97-rp2bUE9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLUFeedForward(nn.Module):\n",
        "  def __init__(self, config: Dict):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.intermediate_size = config.intermediate_size\n",
        "\n",
        "    self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "    self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "    self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.down_proj(F.silu(self.gate_proj(x))*self.up_proj(x))"
      ],
      "metadata": {
        "id": "kgAGtW0tfeBV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config: Dict):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config.hidden_size\n",
        "\n",
        "    self.attention_block = GroupedQueryAttention(config)\n",
        "    self.ffn = SwiGLUFeedForward(config)\n",
        "    self.pre_norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
        "    self.attention_norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, attention_mask: Optional[torch.Tensor]=None):\n",
        "    residual = x\n",
        "    x = self.pre_norm(x)\n",
        "    x = self.attention_block(x, attention_mask)\n",
        "    x += residual\n",
        "\n",
        "    residual = x\n",
        "    x = self.attention_norm(x)\n",
        "    x = self.ffn(x)\n",
        "    x += residual\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "jlnNCYJPgRni"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaMA4(nn.Module):\n",
        "  def __init__(self, config: Dict):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.vocabulary_size = config.vocabulary_size\n",
        "\n",
        "    self.embed_tokens = nn.Embedding(config.vocabulary_size, config.hidden_size)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [TransformerBlock(config) for _ in range(config.num_hidden_layers)]\n",
        "    )\n",
        "    self.norm = RMSNorm(config.hidden_size, config.rms_norm_eps)\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocabulary_size, bias=False)\n",
        "\n",
        "    if config.tie_word_embeddings:\n",
        "      self.lm_head.weight = self.embed_tokens.weight\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    std = self.config.initializer_range\n",
        "    if isinstance(module, nn.Linear):\n",
        "      module.weight.data.normal_(mean=0.0, std=std)\n",
        "      if module.bias is not None:\n",
        "        module.bias.data.zero_()\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "        module.weight.data.normal_(mean=0.0, std=std)\n",
        "\n",
        "  def forward(self, input_ids: torch.Tensor, attention_mask:Optional[torch.Tensor]=None, labels: Optional[torch.Tensor]=None):\n",
        "    hidden_states = self.embed_tokens(input_ids)\n",
        "\n",
        "    if attention_mask is None:\n",
        "      seq_len = input_ids.shape[1]\n",
        "      attention_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      hidden_states = layer(hidden_states, attention_mask=attention_mask)\n",
        "\n",
        "      hidden_states = self.norm(hidden_states)\n",
        "      logits = self.lm_head(hidden_states)\n",
        "\n",
        "      loss = None\n",
        "      if labels is not None:\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        loss = F.cross_entropy(\n",
        "            shift_logits.view(-1, self.vocab_size),\n",
        "            shift_labels.view(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "\n",
        "      return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "    def count_total_params(self):\n",
        "      total_params = sum(p.numel() for p in self.parameters())\n",
        "      trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "      return {\"total\": total_params, \"trainable\": trainable_params}"
      ],
      "metadata": {
        "id": "tbm_Cg-shou1"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}