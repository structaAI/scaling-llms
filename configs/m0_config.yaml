model:
  vocab_size: 32000
  hidden_size: 512
  num_layers: 2
  num_heads: 2
  intermediate_size: 2048
  max_seq_len: 2048
  dropout: 0.1
  rotary_embeddings: true

tokenizer:
  type: bpe
  path: tokenizer/python_bpe.json
  special_tokens: ["<pad>", "<unk>", "<bos>", "<eos>"]

training:
  batch_size: 64
  epochs: 20
  gradient_accumulation_steps: 2
  mixed_precision: bf16
  checkpoint_interval: 5
  save_dir: checkpoints/model_10M
  log_interval: 100

optimizer:
  type: AdamW
  learning_rate: 5e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  eps: 1e-8

scheduler:
  type: cosine
  warmup_steps: 500
  min_lr: 1e-6

curriculum:
  stages:
    - name: syntax
      start_epoch: 0
      end_epoch: 5
      tags: ["syntax", "short"]
      batch_size: 64
    - name: algorithmic
      start_epoch: 5
      end_epoch: 15
      tags: ["algo", "medium"]
      batch_size: 32
    - name: multi-file
      start_epoch: 15
      end_epoch: 20
      tags: ["multi-file", "long"]
      batch_size: 16

dataset:
  train_path: data/processed/python_train.jsonl
  val_path: data/processed/python_val.jsonl
  languages: ["python"]
  shuffle: true

evaluation:
  benchmarks: ["humaneval", "mbpp"]
  metrics: ["compile_rate", "pass@1", "latency", "perplexity"]
  eval_interval: 5

logging:
  tool: wandb
  project: scaling-llms
  run_name: model_10M_v1