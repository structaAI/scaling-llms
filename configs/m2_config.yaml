model:
  vocab_size: 32000
  hidden_size: 1024
  num_layers: 12
  num_heads: 12
  intermediate_size: 4096
  max_seq_len: 2048
  dropout: 0.1
  rotary_embeddings: true

tokenizer:
  type: bpe
  path: tokenizer/python_bpe.json
  special_tokens: ["<pad>", "<unk>", "<bos>", "<eos>"]

training:
  batch_size: 16
  epochs: 35
  gradient_accumulation_steps: 8
  mixed_precision: bf16
  checkpoint_interval: 5
  save_dir: checkpoints/model_100M
  log_interval: 100

optimizer:
  type: AdamW
  learning_rate: 2e-4
  weight_decay: 0.01
  betas: [0.9, 0.95]
  eps: 1e-8

scheduler:
  type: cosine
  warmup_steps: 1500
  min_lr: 1e-6

curriculum:
  stages:
    - name: syntax
      start_epoch: 0
      end_epoch: 5
      tags: ["syntax", "short"]
      batch_size: 32
    - name: algorithmic
      start_epoch: 5
      end_epoch: 20
      tags: ["algo", "medium"]
      batch_size: 16
    - name: advanced
      start_epoch: 20
      end_epoch: 35
      tags: ["multi-file", "long"]
      batch_size: 8

dataset:
  train_path: data/processed/python_train.jsonl
  val_path: data/processed/python_val.jsonl
  languages: ["python"]
  shuffle: true

evaluation:
  benchmarks: ["humaneval", "mbpp", "apps"]
  metrics: ["compile_rate", "pass@1", "latency", "perplexity"]
  eval_interval: 5

logging:
  tool: wandb
  project: scaling-llms
  run_name: model_100M_v1