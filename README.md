# Scaling Language Models for Code Generation

This repository investigates the scaling behavior of LLaMA-based architectures in the 
context of Python code generation. Five models, ranging from 10 million to 7 billion 
parameters, are evaluated to examine how model scale influences code quality, 
efficiency, and emergent reasoning capabilities. The primary objective is to determine 
the minimal viable configuration that exhibits structured and functionally meaningful 
Python code generation. The study analyzes compute-performance trade-offs and 
emergent phenomena associated with model size, providing empirical insights into 
scaling laws for code-oriented large language models (LLMs). The findings aim to 
guide future architectural design and model selection for efficient and accurate code 
generation tasks.

The Official Study for Scaling down LLMs/SLMs, The minimum viable model will be made available for use after the study, Date not confirmed.